{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was gathered as per Udacity instructions from three different sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### twitter_archive_enhanced.csv \n",
    "This provided by Udacity which contain 5000+ tweets from WeRateDogs Twitter archive where is tweet_id is the unique identifier for the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### image_predictions.tsv\n",
    "This is another file provided by Udacity which is the result of neural network prepared by udacity to classify breeds of dogs*. This file has been downloaded programmatically in python notebook using Request library.\"Requests will allow you to send HTTP/1.1 requests using Python. With it, you can add content like headers, form data, multipart files, and parameters via simple Python libraries. It also allows you to access the response data of Python in the same way.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tweet_json.txt\n",
    "This is the file where you store all the tweets extracted using twitter API I extracted all the tweets attributes such as favorite count , retweet count ....etc. The data was reprensted in JSON format and stored as text file encoded using UTF-8 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used .info , .describe and .value_counts to go to each dataset and visually assess it. During the assessing stage I have across the following quality and tidenss issues:\n",
    "\n",
    "##### `twitterArchive_df` table\n",
    "- Change timestamp column from object to date time format\n",
    "\n",
    "- Remove 181 tweets (duplicate rating because it is retweet)\n",
    "\n",
    "- Drop in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id ,expanded_urls,  retweeted_status_user_id and retweeted_status_timestamp because their records are useless due to the lack of information (NAN valuesare not usefull for analysis)\n",
    "\n",
    "- Fix tweet ID 835246439529840640 that has rating_denominator 0 (not correct mathmetically). it should be 13/10\n",
    "\n",
    "- Remove tweet ID 740373189193256964 because the rating is not correct (refer back to 'text' column 9/11 is date not rate)\n",
    "\n",
    "- Remove tweet ID 810984652412424192 because tweet dose not contain rating (user mentioned 24 hours/ 7 days).\n",
    "- Remove tweet id 832088576586297345 since it contains no rating \n",
    "\n",
    "\n",
    "- Fix the rating for tweet id 722974582966214656 (rating 13/10 not 4/20)\n",
    "\n",
    "- Fix the rating for tweet id 666287406224695296 (rating 9/10 not 3/1)\n",
    "\n",
    "- Fix the rating for tweet id 682962037429899265 (rating 10/10 not 7/11)\n",
    "\n",
    "- Fix the rating for tweet id 716439118184652801 (rating 11/10 not 50/50)\n",
    "\n",
    "- Calculate the rating and store in 'rating' column in twitter_archive\n",
    "\n",
    "#### Tidiness\n",
    "- Condensing Dog Type columns\n",
    "- The dog stages have values as columns, instead of one column filled with their values.\n",
    "- Condensing dog breed predictions\n",
    "\n",
    "As you can see above, I have done some extensive cleaning starting by removing the retweets, removing the null value columns (in_reply_to_user_id, retweeted_status_id ...etc.). After that I filtered all the data that has denumerator not equal 10. The previous step allow me to go and check why the denumerator is not equal 10. Interestingly, I found that some tweets rating has been extracted wrongly and I have to refer back to the text field in twitterArchive data frame and read the tweets again to see why the rating ins not correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I finish with assess all the data, I started cleaning the data as per the note written in the assess section and it was awesome to go through each observation and check it out. The following is what I have after cleaning the data:\n",
    "- Dog type column has been created to store dog type instead of the four columns for the same purpose.\n",
    "- Breed columns has been introduced in the image prediction dataset which will condensed all the usefull data in two columns.\n",
    "- Un-usfull columns has been dropped from the each dataset and I ensured data quality by correcting the necessary fields in tweets archive file provided by Udacity.\n",
    "\n",
    "After cleaning the data from each dataset, I have considated all the three data source in one single file which I call it the master data frame of the WeRateDogs tweet account to do further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
